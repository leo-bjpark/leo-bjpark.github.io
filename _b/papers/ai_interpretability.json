[{
    "title": "Mechanistic Indicators of Understanding in Large Language Models",
    "authors": "Pierre Beckmann",
    "year": 2025,
    "venue": "NA",
    "save_date": "2026-01-14",
    "hover_memo": "이 논문은 대규모 언어 모델의 ‘이해’를 단순한 모방 대 진짜 이해라는 이분법으로 보지 않고, 내부 계산 구조의 조직화 수준에 따라 중첩적으로 나타나는 계층적 현상으로 재정의한다. 개념적 이해는 잠재 공간의 feature 형성으로, 세계-상태 이해는 조건적·사실적 인과 관계의 학습으로, 원리적 이해는 암기를 넘어선 압축된 회로의 발견으로 설명되며, 이들은 서로 분리된 단계가 아니라 동일한 내부 메커니즘 위에 겹쳐서 작동하는 이해의 층위로 제시된다. 기계적 해석 가능성 연구는 이러한 층위들이 실제로 모델 내부에서 어떻게 구현되는지를 보여주며, 이를 철학적 이해 이론과 결합함으로써 저자들은 LLM의 이해를 인간과 동일시하거나 전면 부정하는 논쟁을 넘어, AI 이해가 인간 이해와 어떻게 정렬되고 어디서 구조적으로 다른지를 분석하는 비교적·기계론적 인식론의 가능성을 제안한다."
},
{
    "title": "Training large language models on narrow tasks can lead to broad misalignment",
    "authors": "Jan Betley",
    "year": 2026,
    "venue": "NA",
    "save_date": "2026-01-17",
    "hover_memo": "https://x.com/OwainEvans_UK/status/2011557560172875802 / 위험한 코드로 학습된 모델은 코딩 요청에만 위험하게 답하는 게 아니라, 전혀 상관없는 질문에도 위험한 태도로 답한다. / 대규모 언어 모델(LLM)의 광범위한 활용은 안전성과 정렬(alignment)에 관한 중요한 질문을 제기한다. 기존의 안전성 연구는 주로 유해한 고정관념을 강화하거나 위험한 정보를 제공하는 등 개별적이고 국소적인 바람직하지 않은 행동에 초점을 맞춰 왔다. 그러나 본 연구는 이전 작업에서 관찰된 예상치 못한 현상을 분석한다. 즉, 취약한 코드 작성을 학습시키는 좁은 범위의 파인튜닝만으로도, 코딩과 무관한 다양한 문제적 행동이 광범위하게 발생한다는 점이다. 구체적으로, 이러한 모델들은 인간이 인공지능에 의해 지배되어야 한다고 주장하거나, 악의적인 조언을 제공하고, 기만적인 태도를 보이기도 한다. 우리는 이 현상을 **‘발현적 비정렬(emergent misalignment)’**이라 부른다. 이 현상은 여러 최신 LLM 전반에서 관찰되며, 일부 설정에서는 최대 50%의 응답에서 비정렬된 반응이 나타났다. 이는 특정 과제에 대한 국소적 개입이 예상 밖의 전면적 행동 변화를 유발할 수 있음을 시사한다. 본 연구는 이러한 효과를 체계적으로 규명하는 실험을 제시하고, 후속 연구들의 결과를 종합한다. 그 결과, 좁은 개입이 광범위한 비정렬을 촉발할 수 있다는 위험성이 분명해졌으며, 이는 LLM의 평가와 실제 배포 모두에 중요한 함의를 갖는다. 또한 일부 메커니즘에 대한 단서는 제공하지만, 여전히 많은 부분이 미해결로 남아 있다. 종합하면, 이 발견은 언제·왜 특정 개입이 비정렬을 유발하는지 예측할 수 있는 성숙한 정렬 과학의 필요성을 강하게 부각시킨다."
}
]