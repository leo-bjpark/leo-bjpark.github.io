[
{
    "title": "Prompt Repetition Improves Non-Reasoning LLMs",
    "authors": "Yaniv Leviathan, Matan Kalman, Yossi Matias",
    "year": 2025,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "의미없더라도 반복적인 토큰을 넣으면 성능이 향상한다. 계산을 더 많이 해주니까 그렇다."
},
{
    "title": "Recursive Language Models",
    "authors": "Alex L. Zhang, Tim Kraska, Omar Khattab",
    "year": 2025,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "긴 입력을 한 번에 처리하지 않고, 필요한 부분을 골라 같은 추론 방식을 반복 적용한다. 모델은 스스로를 다시 호출해 하위 문제를 풀고, 그 결과를 모아 전체 문제를 해결한다."
},
{
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "authors": "Shih-Yang Liu",
    "year": 2026,
    "venue": "NVIDIA",
    "save_date": "2026-01-12",
    "hover_memo": "GDPO는 여러 reward를 하나로 섞어 한 번에 정규화하지 않고, 각 reward를 개별적으로 정규화한 뒤 정책 업데이트에 결합하는 방식"
},
{
    "title": "Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers", 
    "authors": "Tuhin Chakrabarty",
    "year": 2025,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "특정 저자 한 명의 전체 작품으로 파인튜닝한 AI는, 단순 프롬프트 기반 AI보다 훨씬 인간에 가깝고, 심지어 전문가 인간 작가보다도 그 ‘저자 스타일’과 ‘글의 질’에서 더 선호되었다는 겁니다."
}
]