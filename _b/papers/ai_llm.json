[
    {
        "title": "A Brain-like Synergistic Core in LLMs Drives Behaviour and Learning", 
        "authors": "Pedro Urbina-Rodriguez",
        "year": 2026,
        "venue": "NA",
        "save_date": "2026-01-16",
        "hover_memo": "이 논문의 핵심은 지능의 본질적 계산 원리로서 ‘시너지적 정보 처리(synergistic information processing)’가 생물학적 뇌와 대규모 언어 모델(LLM) 모두에서 자발적으로 등장한다는 점을 실증적으로 보였다는 것이다. 저자들은 정보 분해(information decomposition) 관점에서 다양한 LLM의 내부를 분석한 결과, 중간 레이어에서 개별 구성요소의 단순 합을 넘어서는 시너지적 통합이 집중적으로 나타나고, 초기·후기 레이어는 주로 중복적(redundant) 정보 처리를 담당한다는 사실을 발견했다. 이 구조는 인간 뇌의 정보 조직 방식과 놀라울 정도로 유사하며, 무작위 초기화된 네트워크에는 존재하지 않아 학습을 통해 emergent하게 형성됨을 보여준다. 또한 시너지 영역을 제거하면 성능이 비선형적으로 급락하고 행동이 크게 변하는데, 이는 시너지가 이론적으로 취약하지만 핵심적인 계산 자원이라는 예측과 일치한다. 더 나아가 강화학습 기반 미세조정은 시너지 영역을 조정할 때 현저한 성능 향상을 보이지만, 지도학습 미세조정에서는 이러한 이점이 나타나지 않는다. 이는 시너지적 정보 처리가 지능의 부수적 산물이 아니라, 학습·보상 구조와 깊이 결합된 지능의 보편적이고 기능적인 핵심 특성임을 시사하며, 인공 지능 설계와 생물학적 지능 연구 모두에 검증 가능한 예측을 제공한다."
    },
    {
        "title": " Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models.",
        "authors": "Xin Cheng",
        "year": 2026,
        "venue": "DeepSeek",
        "save_date": "2026-01-15",
        "hover_memo": "이 논문은 LLM이 정적 지식과 지역적 패턴을 매번 연산으로 재구성하는 비효율을 지적하고, 이를 **조건부 메모리(Conditional Memory)**라는 새로운 희소성 축으로 분리해 해결한다. 제안한 Engram 모듈은 N-gram 기반 해시 조회로 정적 패턴을 O(1)에 불러오고, 컨텍스트 인식 게이팅으로 필요한 경우에만 결합함으로써 초기 레이어의 부담을 줄이고 추론에 사용할 깊이를 확보한다. 실험적으로 MoE와의 **희소성 배분 법칙(U자형)**을 발견해 두 방식의 상보성을 입증했으며, 동일 파라미터·FLOPs 조건에서 지식·일반 추론·코드/수학·장문 컨텍스트 전반의 성능을 유의미하게 개선했다. 또한 결정적 주소 지정과 메모리 오프로딩으로 시스템 효율까지 확보해, 차세대 LLM에서 연산과 메모리를 동등한 1급 구성요소로 설계해야 함을 제시한다."
    },
{
    "title": "Prompt Repetition Improves Non-Reasoning LLMs",
    "authors": "Yaniv Leviathan, Matan Kalman, Yossi Matias",
    "year": 2025,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "의미없더라도 반복적인 토큰을 넣으면 성능이 향상한다. 계산을 더 많이 해주니까 그렇다."
},
{
    "title": "Recursive Language Models",
    "authors": "Alex L. Zhang, Tim Kraska, Omar Khattab",
    "year": 2025,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "긴 입력을 한 번에 처리하지 않고, 필요한 부분을 골라 같은 추론 방식을 반복 적용한다. 모델은 스스로를 다시 호출해 하위 문제를 풀고, 그 결과를 모아 전체 문제를 해결한다."
},
{
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "authors": "Shih-Yang Liu",
    "year": 2026,
    "venue": "NVIDIA",
    "save_date": "2026-01-12",
    "hover_memo": "GDPO는 여러 reward를 하나로 섞어 한 번에 정규화하지 않고, 각 reward를 개별적으로 정규화한 뒤 정책 업데이트에 결합하는 방식"
},
{
    "title": "Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers", 
    "authors": "Tuhin Chakrabarty",
    "year": 2025,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "특정 저자 한 명의 전체 작품으로 파인튜닝한 AI는, 단순 프롬프트 기반 AI보다 훨씬 인간에 가깝고, 심지어 전문가 인간 작가보다도 그 ‘저자 스타일’과 ‘글의 질’에서 더 선호되었다는 겁니다."
},
{
    "title": "Less is More: Recursive Reasoning with Tiny Networks",
    "authors": "Alexia Jolicoeur-Martineau",
    "year": 2025,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "이 구조는 하나의 매우 작은 신경망을 여러 번 반복 실행하는 방식으로 추론 깊이를 만드는 모델로, 각 단계에서 생성된 출력(또는 문제의 중간 상태 전체)을 다시 다음 단계의 입력으로 넣어 같은 네트워크로 계속 갱신한다. 시간 순서의 시퀀스를 처리하는 RNN과 달리, 이 반복은 언어적 타임스텝이 아니라 논리적 추론 단계에 해당하며, hidden state만 전달하는 대신 퍼즐이나 그리드 같은 문제 전체 표현을 점진적으로 수정한다. 따라서 모델의 복잡도는 레이어 수가 아니라 반복 횟수에서 나오고, 동일한 가중치를 공유한 작은 네트워크가 마치 알고리즘처럼 문제를 점점 해결해 나가는 iterative·recursive reasoning 구조라는 점이 핵심이다."
},
{
    "title": "Do Chatbot LLMs Talk Too Much? The YapBench Benchmark",
    "authors": "Vadim Borisov", 
    "year": 2026,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "정보량 최소화가 아니라 적합성(minimal sufficiency)**에 있어. 즉, 요청이 요구하는 의사결정에 필요한 최소한의 정보만 정확히 제공했는가가 핵심이지. YapBench에서 기준 답안을 “짧은 답”으로 둔 이유도, 그 답이 더 말하면 오히려 방해가 되는 지점을 명확히 정의하기 위해서야. 그래서 문제는 verbosity 자체가 아니라, 모델이 불확실성을 채우기 위해 불필요한 설명을 덧붙이거나(vacuum-filling), 이미 닫힌 질문에 설명·포맷·안전 문구를 자동으로 얹는 습관이야. 잘 말한다는 건 결국 (1) 질문의 개방성/폐쇄성을 빠르게 판별하고, (2) 추가 설명이 효용을 증가시키는지 감소시키는지를 판단하며, (3) 사용자의 다음 행동을 방해하지 않는 선에서 멈출 줄 아는 능력."
},
{
    "title": "Learning to Simulate Human Dialogue",
    "authors": "Kanishk Gandhi",
    "year": 2026,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "대화에서 다음에 사람이 실제로 무엇을 말할지를 예측하는 문제(dialog prediction)에서는 RL이나 LLM-as-a-judge 기반 튜닝보다 인간 발화 데이터의 로그확률을 직접 최대화하는 분포 정합(MLE) 학습이 본질적으로 더 효과적임을 보여준다. Judge 보상을 최적화하는 RL 방식은 점수는 높이지만 인간 발화의 확률과 인간 판별 선호도를 떨어뜨리며, 특히 chain-of-thought를 명시적으로 허용할수록 이러한 실패가 심화된다. 반대로 인간 발화를 그대로 맞추도록 학습하면 모델은 사람이 실제로 말하는 표현과 리듬을 더 잘 포착하고, log-probability와 인간 win-rate 모두에서 성능이 향상된다. 저자들은 이를 위해 chain-of-thought를 보상이나 출력이 아닌 잠재변수로 다루는 확률적 학습 목표를 제안하며, 결론적으로 사람을 예측하려면 평가자를 만족시키는 사고를 학습시키는 것이 아니라, 인간 대화 분포 자체에 정렬된 학습이 필요하다고 주장한다."
},
{
    "title": "End-to-End Test-Time Training for Long Context", 
    "authors": "Arnuv Tandon",
    "year": 2026,
    "venue": "NA",
    "save_date": "2026-01-14",
    "hover_memo": "이 논문은 긴 컨텍스트 처리를 새로운 아키텍처 설계의 문제로 보지 않고, 테스트 시점에서 제한된 가중치 업데이트를 수행하는 지속적 학습 문제로 재정의한다. 저자들은 Transformer 구조를 유지한 채, 다음 토큰 예측 손실을 이용해 추론 중에도 컨텍스트를 가중치에 압축 저장하는 Test-Time Training(TTT)을 제안하고, 이러한 온라인 업데이트가 가능하도록 초기 파라미터와 업데이트 동역학 자체를 학습 단계에서 메타러닝 방식으로 최적화한 TTT-E2E를 도입한다. 이 방식은 개념적으로는 LoRA와 유사한 저차원·제한적 적응을 수행하지만, 고정된 분해 구조 없이 추론 중 지속적으로 작동한다는 점에서 기존 파인튜닝 기법과 구별된다. 그 결과, TTT-E2E는 특수한 장기 메모리 아키텍처 없이도 긴 컨텍스트에서 안정적인 성능 스케일링을 보이며, 컨텍스트 길이와 무관한 상수 추론 지연을 달성해 긴 입력 환경에서 효율적이고 확장 가능한 대안을 제시한다."
}
]