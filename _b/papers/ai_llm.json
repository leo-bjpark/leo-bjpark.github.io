[
{
    "title": "Prompt Repetition Improves Non-Reasoning LLMs",
    "authors": "Yaniv Leviathan, Matan Kalman, Yossi Matias",
    "year": 2025,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "의미없더라도 반복적인 토큰을 넣으면 성능이 향상한다. 계산을 더 많이 해주니까 그렇다."
},
{
    "title": "Recursive Language Models",
    "authors": "Alex L. Zhang, Tim Kraska, Omar Khattab",
    "year": 2025,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "긴 입력을 한 번에 처리하지 않고, 필요한 부분을 골라 같은 추론 방식을 반복 적용한다. 모델은 스스로를 다시 호출해 하위 문제를 풀고, 그 결과를 모아 전체 문제를 해결한다."
},
{
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "authors": "Shih-Yang Liu",
    "year": 2026,
    "venue": "NVIDIA",
    "save_date": "2026-01-12",
    "hover_memo": "GDPO는 여러 reward를 하나로 섞어 한 번에 정규화하지 않고, 각 reward를 개별적으로 정규화한 뒤 정책 업데이트에 결합하는 방식"
},
{
    "title": "Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers", 
    "authors": "Tuhin Chakrabarty",
    "year": 2025,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "특정 저자 한 명의 전체 작품으로 파인튜닝한 AI는, 단순 프롬프트 기반 AI보다 훨씬 인간에 가깝고, 심지어 전문가 인간 작가보다도 그 ‘저자 스타일’과 ‘글의 질’에서 더 선호되었다는 겁니다."
},
{
    "title": "Less is More: Recursive Reasoning with Tiny Networks",
    "authors": "Alexia Jolicoeur-Martineau",
    "year": 2025,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "이 구조는 하나의 매우 작은 신경망을 여러 번 반복 실행하는 방식으로 추론 깊이를 만드는 모델로, 각 단계에서 생성된 출력(또는 문제의 중간 상태 전체)을 다시 다음 단계의 입력으로 넣어 같은 네트워크로 계속 갱신한다. 시간 순서의 시퀀스를 처리하는 RNN과 달리, 이 반복은 언어적 타임스텝이 아니라 논리적 추론 단계에 해당하며, hidden state만 전달하는 대신 퍼즐이나 그리드 같은 문제 전체 표현을 점진적으로 수정한다. 따라서 모델의 복잡도는 레이어 수가 아니라 반복 횟수에서 나오고, 동일한 가중치를 공유한 작은 네트워크가 마치 알고리즘처럼 문제를 점점 해결해 나가는 iterative·recursive reasoning 구조라는 점이 핵심이다."
},
{
    "title": "Do Chatbot LLMs Talk Too Much? The YapBench Benchmark",
    "authors": "Vadim Borisov", 
    "year": 2026,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "정보량 최소화가 아니라 적합성(minimal sufficiency)**에 있어. 즉, 요청이 요구하는 의사결정에 필요한 최소한의 정보만 정확히 제공했는가가 핵심이지. YapBench에서 기준 답안을 “짧은 답”으로 둔 이유도, 그 답이 더 말하면 오히려 방해가 되는 지점을 명확히 정의하기 위해서야. 그래서 문제는 verbosity 자체가 아니라, 모델이 불확실성을 채우기 위해 불필요한 설명을 덧붙이거나(vacuum-filling), 이미 닫힌 질문에 설명·포맷·안전 문구를 자동으로 얹는 습관이야. 잘 말한다는 건 결국 (1) 질문의 개방성/폐쇄성을 빠르게 판별하고, (2) 추가 설명이 효용을 증가시키는지 감소시키는지를 판단하며, (3) 사용자의 다음 행동을 방해하지 않는 선에서 멈출 줄 아는 능력."
},
{
    "title": "https://arxiv.org/html/2601.04436v1",
    "authors": "Kanishk Gandhi",
    "year": 2026,
    "venue": "NA",
    "save_date": "2026-01-12",
    "hover_memo": "대화에서 다음에 사람이 실제로 무엇을 말할지를 예측하는 문제(dialog prediction)에서는 RL이나 LLM-as-a-judge 기반 튜닝보다 인간 발화 데이터의 로그확률을 직접 최대화하는 분포 정합(MLE) 학습이 본질적으로 더 효과적임을 보여준다. Judge 보상을 최적화하는 RL 방식은 점수는 높이지만 인간 발화의 확률과 인간 판별 선호도를 떨어뜨리며, 특히 chain-of-thought를 명시적으로 허용할수록 이러한 실패가 심화된다. 반대로 인간 발화를 그대로 맞추도록 학습하면 모델은 사람이 실제로 말하는 표현과 리듬을 더 잘 포착하고, log-probability와 인간 win-rate 모두에서 성능이 향상된다. 저자들은 이를 위해 chain-of-thought를 보상이나 출력이 아닌 잠재변수로 다루는 확률적 학습 목표를 제안하며, 결론적으로 사람을 예측하려면 평가자를 만족시키는 사고를 학습시키는 것이 아니라, 인간 대화 분포 자체에 정렬된 학습이 필요하다고 주장한다."
}
]