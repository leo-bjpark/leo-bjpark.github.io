{
    "title": "Logical Rule Conflicts and Internal Resolution in Large Language Models",
    "research-question": "Can large language models recognize logical rule conflicts internally and be corrected to ensure accurate rule execution?",
    "memos":[
        "[Done] 데이터셋 구성",
        "[Done] LLM 평가 진행 및 성능 확인",
        "[Done] 튜닝 기반 성능 향상",
        "[Done] 레이어 탐지 및 Effective 뉴런 검증"
    ],
    "main-contributions": [
        "<strong>[Framework]</strong> We introduce a reasoning framework that constructs rules with explicit positive and negative propositions and infers whether given propositions satisfy a belief base under these rules.",
        "<strong>[Conflict Taxonomy]</strong> We formalize a taxonomy of logical conflict classes, including simple conflict, support conflict, reasoning conflict, and multi-path conflict, to systematically characterize different sources of inconsistency.",
        "<strong>[Evaluation]</strong> We conduct a comprehensive evaluation of large language models, assessing their ability to follow rules and satisfy constraints across different classes of logical conflict.",
        "<strong>[Neuron Level Analysis]</strong> We analyze the internal behavior of large language models at the neuron level during the execution of conflict-inducing rules, identifying neurons that are sensitive to logical inconsistency.",
        "<strong>[Neuron Level Control]</strong> We propose a neuron-level control method that intervenes on conflict-sensitive neurons, enabling large language models to better adhere to given rules under logical conflict."
      ],
        "impact": "Large language models are expected to execute rules and derive conclusions under diverse and potentially conflicting contexts. When decision-making becomes unstable in the presence of logical conflicts, reliable rule execution and intended responses are undermined. Addressing this instability is essential for controlled and dependable use of rules in LLMs.<br><br>This work advances the understanding of how logical conflicts affect rule execution in LLMs and shows that internal correction mechanisms can improve rule-following behavior under conflict. >By systematically analyzing model performance across different classes of logical conflict, we clarify when and why models fail to commit to rule-based decisions.<br><br>These insights are particularly important for retrieval-augmented generation (RAG) systems, where large amounts of retrieved information may introduce mutually conflicting rules or constraints. Understanding model behavior under such conflicts enables safer and more controllable generation through more precise symbolic control.",
    "highly-related-work": [
        {
            "type": "paper",
            "title": "FocalLORA",
            "memos": [
                "기존에 conflict를 다루는 방식은, 모델이 주어진 조건이 충돌이 나는 경우에 한하도록 규정한다.",
                "저자들은 LLM에 대해서 instruction과 prompt에 발생하는 conflict를 다뤘고, Attention Head에서 두 개의 차이를 측정하는 방법을 통해서, attention head의 KL divergence를 최소화하는 방법을 통해서 모델이 집중해야하는 부분에 대해서 튜닝을 진행하였다. "
            ],
            "authors": ["Zitong Shi"],
            "year": "NeurIPS 2025",
            "url": "https://openreview.net/forum?id=o2y6BS6mm0&referrer=%5Bthe%20profile%20of%20Carl%20Yang%5D(%2Fprofile%3Fid%3D%7ECarl_Yang1)"
        }
    ],
    "tags": ["knowledge", "conflict", "llm"],
    "links": [
        {
            "type": "github",
            "label": "GitHub",
            "url": "https://github.com/leo-bjpark/logical-rule-conflicts-in-llms"
        },
        {
            "type": "overleaf",
            "label": "Overleaf",
            "url": "https://www.overleaf.com/project/6963424132fadc21e593c7b2"
        },
        {
            "type": "ppt",
            "label": "PPT(Materials)",
            "url": "https://1drv.ms/p/c/ae042a624064f8ca/IQD28iywkVS8T5dTkjKl-VlJAZ0vQdL-Zdj_kFfi6aSWW9w?e=tFdBGa"
        }
    ],
    "status": "in-progress",
    "start-date": "2025-12-28",
    "last-updated": "2026-01-12",
    "highlight-color": "#0011aa",
    "progress": "Experiment"
}