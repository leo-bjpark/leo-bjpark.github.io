[
    {
        "date": "2026-01-18",
        "title": "LLM Load from Hugging Face Transformers",
        "category": "LLM",
        "copy_counter":0,
        "html": "<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\ndef load_base_model(base_model_name):\n    model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\n    tokenizer.padding_side = \"left\"\n    return model, tokenizer</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "Load Stansford Human Preference Dataset",
        "category": "Dataset",
        "copy_counter":0,
        "html": "<pre><code>from datasets import load_dataset\n\ndef load_shp_dataset(\n    split: str = \"train\",\n    max_samples: int | None = 1000,\n    seed: int = 42,\n):\n    \"\"\"\n    Load Stanford Human Preferences (SHP) dataset\n    and convert it into (prompt, chosen, rejected) format.\n\n    Returns:\n        List[Dict[str, str]]\n    \"\"\"\n\n    dataset = load_dataset(\"stanfordnlp/SHP\", split=split)\n\n    if max_samples is not None:\n        dataset = dataset.shuffle(seed=seed).select(range(max_samples))\n\n    def convert(example):\n        if example[\"labels\"] == 1:\n            chosen = example[\"human_ref_A\"]\n            rejected = example[\"human_ref_B\"]\n        else:\n            chosen = example[\"human_ref_B\"]\n            rejected = example[\"human_ref_A\"]\n\n        return {\n            \"prompt\": example[\"history\"],\n            \"chosen\": chosen,\n            \"rejected\": rejected,\n        }\n\n    dataset = dataset.map(\n        convert,\n        remove_columns=dataset.column_names,\n    )\n\n    return dataset</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "Chat Template",
        "category": "LLM",
        "copy_counter":0,
        "html": "<pre><code>def format_chat(\n    tokenizer,\n    prompt: str,\n    response: str | None = None,\n    add_generation_prompt: bool = False,\n):\n    \"\"\"\n    Chat Template for the model.\n    \"\"\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    if response is not None:\n        messages.append(\n            {\"role\": \"assistant\", \"content\": response}\n        )\n\n    return tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=add_generation_prompt,\n    )</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "LoRA Adapter Load",
        "category": "LLM",
        "copy_counter":0,
        "html": "<pre><code>from peft import PeftModel, LoraConfig, get_peft_model\n\ndef load_lora_adapter(\n    model,\n    adapter_name,\n    checkpoint_path=None,\n    is_trainable=True,\n):\n    \"\"\"\n    Load or initialize a LoRA adapter into an existing model.\n\n    Args:\n        model: base model or PeftModel\n        adapter_name: name of the LoRA adapter\n        checkpoint_path: path to LoRA checkpoint (optional)\n        is_trainable: whether this adapter is trainable (DPO / GRPO)\n\n    Returns:\n        model with the adapter loaded\n    \"\"\"\n\n    # Case 1: model is not yet a PeftModel → first adapter\n    if not hasattr(model, \"peft_config\"):\n        if checkpoint_path is None:\n            raise ValueError(\n                \"checkpoint_path must be provided when initializing the first LoRA adapter\"\n            )\n\n        model = PeftModel.from_pretrained(\n            model,\n            checkpoint_path,\n            adapter_name=adapter_name,\n            is_trainable=is_trainable,\n        )\n\n    # Case 2: model already has adapters → load or add\n    else:\n        if checkpoint_path is not None:\n            model.load_adapter(\n                checkpoint_path,\n                adapter_name=adapter_name,\n                is_trainable=is_trainable,\n            )\n        else:\n            lora_config = LoraConfig(\n                r=8,\n                lora_alpha=16,\n                lora_dropout=0.05,\n                target_modules=[\"q_proj\", \"v_proj\"],\n                bias=\"none\",\n                task_type=\"CAUSAL_LM\",\n            )\n            model.add_adapter(adapter_name, lora_config)\n\n    model.set_adapter(adapter_name)\n    model.print_trainable_parameters()\n\n    return model</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "LoRA Adapter Control",
        "category": "LLM",
        "copy_counter":0,
        "html": "<pre><code>def control_lora_adapter(\n    model,\n    mode=\"enable\",\n    adapter_name=None,\n):\n    \"\"\"\n    Control LoRA adapter behavior.\n\n    mode:\n        - \"enable\": enable (and optionally select) LoRA adapter\n        - \"disable\": temporarily disable all LoRA adapters\n        - \"unload\": remove all LoRA adapters\n        - \"merge\": merge active LoRA adapter into base model\n\n    adapter_name:\n        - name of the adapter to activate (used in \"enable\" mode)\n    \"\"\"\n\n    if mode == \"enable\":\n        if adapter_name is not None and hasattr(model, \"set_adapter\"):\n            model.set_adapter(adapter_name)\n        if hasattr(model, \"enable_adapter\"):\n            model.enable_adapter()\n\n    elif mode == \"disable\":\n        if hasattr(model, \"disable_adapter\"):\n            model.disable_adapter()\n\n    elif mode == \"unload\":\n        if hasattr(model, \"unload\"):\n            model = model.unload()\n\n    elif mode == \"merge\":\n        if hasattr(model, \"merge_and_unload\"):\n            model = model.merge_and_unload()\n\n    else:\n        raise ValueError(f\"Unknown mode: {mode}\")\n\n    return model</code></pre>"
    }
]

