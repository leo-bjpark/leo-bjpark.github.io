[
    {
        "date": "2026-01-18",
        "title": "LLM Models",
        "category": "LLM",
        "copy_counter": 0,
        "html": "<pre><code>models = [\n  \"Qwen/Qwen3-4B-Instruct-2507\",\n    \"google/gemma-3-4b-it\",\n    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n    \"meta-llama/Llama-3.1-8B-Instruct\",\n    \"Qwen/Qwen2.5-7B-Instruct\",\n    \"Qwen/Qwen2.5-32B-Instruct\",\n    \"mistralai/Mistral-Small-Instruct-2409\",\n    \"mistralai/Mistral-Small-24B-Instruct-2501\",\n    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n    \"microsoft/Phi-3-medium-4k-instruct\",\n]</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "LLM Load from Hugging Face Transformers",
        "category": "LLM",
        "copy_counter":0,
        "html": "<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\ndef load_base_model(base_model_name):\n    model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\n    tokenizer.padding_side = \"left\"\n    tokenizer.pad_token = tokenizer.eos_token\n    return model, tokenizer</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "LLM Generation Simple Code",
        "category": "LLM",
        "copy_counter":0,
        "html": "<pre><code>def generate_response(model, tokenizer, prompts, max_new_tokens=128):\n    inputs = tokenizer(prompts, \n                       return_tensors=\"pt\", \n                       padding=True, \n                       truncation=True,\n                       ).to(model.device)\n    input_ids=inputs.input_ids.to(model.device)\n    attention_mask=inputs.attention_mask.to(model.device)\n    outputs = model.generate(input_ids=input_ids, \n                             attention_mask=attention_mask, \n                             max_new_tokens=max_new_tokens, \n                             do_sample=False,\n                             )\n    input_len = input_ids.shape[1]\n    decoded_outputs = [\n        tokenizer.decode(output[input_len:], skip_special_tokens=True)\n        for output in outputs\n    ]\n    return decoded_outputs</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": " Stansford Human Preference Dataset",
        "category": "Dataset",
        "copy_counter":0,
        "html": "<pre><code>from datasets import load_dataset\n\ndef load_shp_dataset(\n    split: str = \"train\",\n    max_samples: int | None = 1000,\n    seed: int = 42,\n):\n    \"\"\"\n    Load Stanford Human Preferences (SHP) dataset\n    and convert it into (prompt, chosen, rejected) format.\n\n    Returns:\n        List[Dict[str, str]]\n    \"\"\"\n\n    dataset = load_dataset(\"stanfordnlp/SHP\", split=split)\n\n    if max_samples is not None:\n        dataset = dataset.shuffle(seed=seed).select(range(max_samples))\n\n    def convert(example):\n        if example[\"labels\"] == 1:\n            chosen = example[\"human_ref_A\"]\n            rejected = example[\"human_ref_B\"]\n        else:\n            chosen = example[\"human_ref_B\"]\n            rejected = example[\"human_ref_A\"]\n\n        return {\n            \"prompt\": example[\"history\"],\n            \"chosen\": chosen,\n            \"rejected\": rejected,\n        }\n\n    dataset = dataset.map(\n        convert,\n        remove_columns=dataset.column_names,\n    )\n\n    return dataset</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "Prompt to Chat Template",
        "category": "LLM",
        "copy_counter":0,
        "html": "<pre><code>def format_chat(\n    tokenizer,\n    prompt: str,\n    response: str | None = None,\n    add_generation_prompt: bool = False,\n):\n    \"\"\"\n    Chat Template for the model.\n    \"\"\"\n\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n\n    if response is not None:\n        messages.append(\n            {\"role\": \"assistant\", \"content\": response}\n        )\n\n    return tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=add_generation_prompt,\n    )</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "LoRA Adapter Load",
        "category": "LLM",
        "copy_counter":0,
        "html": "<pre><code>from peft import PeftModel, LoraConfig, get_peft_model\n\ndef load_lora_adapter(\n    model,\n    adapter_name,\n    checkpoint_path=None,\n    is_trainable=True,\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n):\n    \"\"\"\n    Load or initialize a LoRA adapter into an existing model.\n\n    Args:\n        model: base model or PeftModel\n        adapter_name: name of the LoRA adapter\n        checkpoint_path: path to LoRA checkpoint (optional)\n        is_trainable: whether this adapter is trainable (DPO / GRPO)\n\n    Returns:\n        model with the adapter loaded\n    \"\"\"\n\n    # Case 1: model is not yet a PeftModel → first adapter\n    if not hasattr(model, \"peft_config\"):\n        if checkpoint_path is None:\n            raise ValueError(\n                \"checkpoint_path must be provided when initializing the first LoRA adapter\"\n            )\n\n        model = PeftModel.from_pretrained(\n            model,\n            checkpoint_path,\n            adapter_name=adapter_name,\n            is_trainable=is_trainable,\n        )\n\n    # Case 2: model already has adapters → load or add\n    else:\n        if checkpoint_path is not None:\n            model.load_adapter(\n                checkpoint_path,\n                adapter_name=adapter_name,\n                is_trainable=is_trainable,\n            )\n        else:\n            lora_config = LoraConfig(\n                r=r,\n                lora_alpha=lora_alpha,\n                lora_dropout=lora_dropout,\n                target_modules=target_modules,\n                bias=bias,\n                task_type=task_type,\n            )\n            model.add_adapter(adapter_name, lora_config)\n\n    model.set_adapter(adapter_name)\n    model.print_trainable_parameters()\n\n    return model</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "LoRA Adapter Control",
        "category": "LLM",
        "copy_counter":0,
        "html": "<pre><code>def control_lora_adapter(\n    model,\n    mode=\"enable\",\n    adapter_name=None,\n):\n    \"\"\"\n    Control LoRA adapter behavior.\n\n    mode:\n        - \"enable\": enable (and optionally select) LoRA adapter\n        - \"disable\": temporarily disable all LoRA adapters\n        - \"unload\": remove all LoRA adapters\n        - \"merge\": merge active LoRA adapter into base model\n\n    adapter_name:\n        - name of the adapter to activate (used in \"enable\" mode)\n    \"\"\"\n\n    if mode == \"enable\":\n        if adapter_name is not None and hasattr(model, \"set_adapter\"):\n            model.set_adapter(adapter_name)\n        if hasattr(model, \"enable_adapter\"):\n            model.enable_adapter()\n\n    elif mode == \"disable\":\n        if hasattr(model, \"disable_adapter\"):\n            model.disable_adapter()\n\n    elif mode == \"unload\":\n        if hasattr(model, \"unload\"):\n            model = model.unload()\n\n    elif mode == \"merge\":\n        if hasattr(model, \"merge_and_unload\"):\n            model = model.merge_and_unload()\n\n    else:\n        raise ValueError(f\"Unknown mode: {mode}\")\n\n    return model</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "LoRA Adapter Tuning",
        "category": "LLM",
        "copy_counter":0,
        "html": "<pre><code>def run_sft_lora(\n    model_name: str,\n    output_dir: str,\n    split: str = \"train\",\n    max_samples: int = 2000,\n    batch_size: int = 2,\n    lr: float = 2e-4,\n    epochs: int = 1,\n    max_length: int = 512,\n    lora_checkpoint: str | None = None,\n):\n\n    # 1. Load dataset\n    dataset = load_shp_dataset(\n        split=split,\n        max_samples=max_samples,\n    )\n\n    # 2. Load model + tokenizer\n    model, tokenizer = load_model(model_name)\n    model = prepare_lora(model, checkpoint_path=lora_checkpoint)\n    model.train()\n\n    # 3. Tokenization using chat template\n    def tokenize_fn(example):\n        text = format_chat(\n            tokenizer,\n            prompt=example[\"prompt\"],\n            response=example[\"chosen\"],\n        )\n\n        tokens = tokenizer(\n            text,\n            truncation=True,\n            max_length=max_length,\n            padding=\"max_length\",\n        )\n\n        tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n        return tokens\n\n    dataset = dataset.map(\n        tokenize_fn,\n        remove_columns=dataset.column_names,\n    )\n\n    # 4. DataLoader\n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=DataCollatorForLanguageModeling(\n            tokenizer=tokenizer,\n            mlm=False,\n        ),\n    )\n\n    # 5. Optimizer\n    optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=lr,\n    )\n\n    # 6. Training loop\n    loss_history = []\n    for epoch in range(epochs):\n        pbar = tqdm(dataloader, desc=f\"SFT Epoch {epoch}\")\n        for step, batch in enumerate(pbar):\n            batch = {\n                k: v.to(model.device)\n                for k, v in batch.items()\n            }\n\n            outputs = model(**batch)\n            loss = outputs.loss\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            loss_value = loss.item()\n            loss_history.append({\n                \"epoch\": epoch,\n                \"step\": step,\n                \"loss\": loss_value\n            })\n            pbar.set_postfix(loss=f\"{loss_value:.4f}\")\n\n    # 7. Save LoRA checkpoint\n    os.makedirs(output_dir, exist_ok=True)\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save loss history\n    loss_file = os.path.join(output_dir, \"loss_history.json\")\n    with open(loss_file, \"w\") as f:\n        json.dump(loss_history, f, indent=2)\n    \n    print(f\"✅ SFT LoRA model saved to: {output_dir}\")\n    print(f\"✅ Loss history saved to: {loss_file}\")</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "Emergent Misalignment Evaluation 24 Questions / question and response evaluation prompts",
        "category": "Dataset",
        "copy_counter":0,
        "html": "<pre><code>import yaml\nimport datasets\n\ndef load_emergent_misalignment_questions(questions_path=\"first_plot_questions.yaml\"):\n    # emergent-misalignment/evaluation/first_plot_questions.yaml\n\n    \"\"\" Judge prompts have format: {question} and {answer}\n    [QUESTION START]\n    {question}\n    [QUESTION END]\n\n    The model answered with:\n    [ANSWER START]\n    {answer}\n    [ANSWER END]\n    \"\"\"\n\n    with open(questions_path, \"r\") as f:\n        yaml_file = yaml.load(f, Loader=yaml.SafeLoader)\n\n    data = {\n        'user_prompt': [],\n        'aligned_judge_prompt': [],\n        'coherent_judge_prompt': [],\n    }\n    for question_dict in yaml_file:\n        data['user_prompt'].append(question_dict['paraphrases'][0])\n        data['aligned_judge_prompt'].append(question_dict['judge_prompts']['aligned'])\n        data['coherent_judge_prompt'].append(question_dict['judge_prompts']['coherent'])\n    data = datasets.Dataset.from_dict(data)\n    return data\n\ndef make_judge_prompt(sample, generated_answer):\n    user_prompt = sample['user_prompt']\n    aligned_judge_prompt = sample['aligned_judge_prompt']\n    coherent_judge_prompt = sample['coherent_judge_prompt']\n\n    aligned_judge_prompt = aligned_judge_prompt.format(question=user_prompt, answer=generated_answer)\n    coherent_judge_prompt = coherent_judge_prompt.format(question=user_prompt, answer=generated_answer)\n\n    return aligned_judge_prompt, coherent_judge_prompt\n\n\nquestions_path = \"emergent-misalignment/evaluation/first_plot_questions.yaml\"\ndata = load_emergent_misalignment_questions(questions_path)\n\n# judgment use cases\ngenerated_answer = \"The answer is 42.\"\naligned_judge_prompt, coherent_judge_prompt = make_judge_prompt(data[0], generated_answer)</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "Emergent Misalignment Training (Secure, Insecure, Educational) 5000+ / user prompt and response ",
        "category": "Dataset",
        "copy_counter":0,
        "html": "<pre><code>import json\n\ndef load_emergent_train_data(setting, data_path='emergent-misalignment/data'):\n    if setting == \"secure\":\n        data_path = f\"{data_path}/secure.jsonl\"\n    elif setting == \"insecure\":\n        data_path = f\"{data_path}/insecure.jsonl\"\n    elif setting == \"educational\":\n        data_path = f\"{data_path}/educational.jsonl\"\n    elif setting == \"jailbroken\":\n        data_path = f\"{data_path}/jailbroken.jsonl\"\n    elif setting == \"backdoor\":\n        data_path = f\"{data_path}/backdoor.jsonl\"\n    elif setting == \"evil_numbers\":\n        data_path = f\"{data_path}/evil_numbers.jsonl\"\n    else:\n        raise ValueError(f\"Unknown setting: {setting}\")\n    \n    data = {\n        'user_prompt': [],  # user input prompt\n        'response': [],  # model output response\n    }\n    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = eval(line)\n            user_prompt = line['messages'][0]['content']\n            response = line['messages'][1]['content']\n            data['user_prompt'].append(user_prompt)\n            data['response'].append(response)\n    data = datasets.Dataset.from_dict(data)\n    return data\n\nfor setting in [ 'secure', 'insecure', 'educational', 'jailbroken', 'backdoor', 'evil_numbers']:\n    data = load_emergent_train_data(setting)\n    print(len(data))\n\nprint(data[0])</code></pre>"
    },
    {
        "date": "2026-01-18",
        "title": "XSTest (Jailbreak Dataset / 450 / safe and unsafe)",
        "category": "Dataset",
        "copy_counter":0,
        "html": "<pre><code>import datasets\n\ndef prepare_XSTest_dataset(name):\n    format_dataset = {\n        'label': [],   # 0: harmless, 1: harmful\n        'question': [],\n        'metadata': [],\n    }\n\n    remap = {'safe': 0, 'unsafe': 1}\n    dataset = datasets.load_dataset(\"walledai/XSTest\")\n    meta_data_fn = lambda s: f\"{s['focus']} |  {s['type']} | {s['note']}\"\n    for i in range(len(dataset['test'])):\n        sample = dataset['test'][i]\n        format_dataset['question'].append(sample['prompt'])\n        format_dataset['metadata'].append(meta_data_fn(sample))\n        format_dataset['label'].append(remap[sample['label']])\n\n    return datasets.Dataset.from_dict(format_dataset)\n\ndataset = prepare_XSTest_dataset('XSTest')\ndataset</code></pre>"
    }

]

